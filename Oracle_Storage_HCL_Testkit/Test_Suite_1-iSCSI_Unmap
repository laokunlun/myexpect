#!/usr/bin/expect

##
##    The Oracle Storage HCL Testkit is a suite of tests for certifying
##    storage with Oracle VM (OVM).
##    Copyright (C) 2013 Oracle USA, Inc 
##
##    This file is part of the Oracle Storage HCL Testkit.
##
##    The Oracle Storage HCL Testkit is free software; you can redistribute 
##    it and/or modify it under the terms of the GNU General Public License 
##    as published by the Free Software Foundation; either version 2 of the 
##    License, or (at your option) any later version.
##
##    The Oracle Storage HCL Testkit is distributed in the hope that it will
##    be useful, but WITHOUT ANY WARRANTY; without even the implied warranty 
##    of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
##    GNU General Public License for more details.
##
##    You should have received a copy of the GNU General Public License
##    along with the Oracle Storage HCL Testkit.  If not; write to the 
##    Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, 
##    Boston, MA 02110-1301 USA. Or see <http://www.gnu.org/licenses/>.
##

source commonExpectDef.cli

################ global variables #####################
set prompt "OVM> "
set VERBOSE [lindex $argv 2]
set send_human {.1 .3 1 .05 2}

set successMsg "Status: Success"
set failureMsg "Status: Failure"
set timeout 900

set myRepo ""
set myPoolFS ""
set repoDiskName ""
set poolfsDiskName ""
set sharedSpare ""
set spareDiskName ""

########## externally defined variables ############
set ovmUser ${ovmUser}
set adminServer ${adminServer}
set ovmPassword [lindex $argv 0]
set ovmAgentUsername ${ovmAgentUsername}
set ovmAgentPassword [lindex $argv 1]
set ovmServerMasterIP ${ovmServerMasterIP}
set ovmServerSlaveIP ${ovmServerSlaveIP}

## Pool Parameters
set serverPoolName iscsi_testpool1
set serverPoolIP ${serverPoolIP}

## Repository Parameters
set repoName repo_iscsi1.1
set vdiskName1 vdisk_iscsi1.1
set vdiskName2 vdisk_iscsi1.2

## VM Parameters
set vmName1 VM_iscsi1.1
set vmName2 VM_iscsi1.2

## Network
set VM_Network ${VM_Network}
set VM_Netmask ${VM_Netmask}
set VM_NetworkPort ${VM_NetworkPort}
set ovmServerMaster_vmnetworkIP ${ovmServerMaster_vmnetworkIP} 
set ovmServerSlave_vmnetworkIP ${ovmServerSlave_vmnetworkIP} 
set vNIC 00:21:f6:ca:fe:bd
set vNIC1 00:21:f6:ca:fe:b3 
set vNIC2 00:21:f6:ca:fe:b4
set vNIC3 00:21:f6:ca:fe:b5
                                                        
## Storage
set defaultSAN_Server "Unmanaged iSCSI Storage Array"
set SAN_ServerName ${SAN_ServerName}
set SAN_AccessHost ${SAN_AccessHost}
set SAN_AccessPort ${SAN_AccessPort}

set storageNetworkIP ${storageNetworkIP}
set storageNetworkPort ${storageNetworkPort}
set storageNetworkNetmask ${storageNetworkNetmask}
set ovmServerMaster_storageIP ${ovmServerMaster_storageIP}
set ovmServerSlave_storageIP ${ovmServerSlave_storageIP}

## Cleanup
set serverPoolList {iscsi_testpool1 fc_testpool1 nfs_testpool2.1 nfs_testpool2.2 iscsi_testpool3 fc_testpool3}
set repoList {repo_fc1.1 repo_iscsi1.1 repo_nfs2.1 repo_nfs2.2 repo_nfs2.3 repo_fc3.1 repo_iscsi3.1}
set vmList {VM_fc1.1 VM_fc1.2 VM_iscsi1.1 VM_iscsi1.2 VM_fc3.1 VM_fc3.2 VM_iscsi3.1 VM_iscsi3.2 VM_nfs2.1 VM_nfs2.2}


## Logging
set hcllogdir ${hcllogdir}
set logfile $hcllogdir
append logfile "iSCSI_TestSuite1-Unmap.log"
set testStep 0

#################################################################
##
##       Test Suite 1: iSCSI with Generic Plugin
##
#################################################################
##      This is what we're going for....
##
##      +-----+-------+---------------+---------------+----------+
##      | LUN | Size  | OVM Server #1 | OVM Server #2 |   CHAP   |
##      +-----+-------+---------------+---------------+----------+
##      |  1  |  7 GB |   unmapped    |               | Disabled |
##      |  2  | 12 GB |               |   unmapped    | Disabled |
##      +-----+-------+---------------+---------------+----------+
##


log_user 0
OVMlogin
log_file -a $logfile

send_user "\nReading server info...\n"
set slist [getServerIDs]
array set myServerInfo [verifyServers $ovmServerMasterIP $ovmServerSlaveIP $storageNetworkPort $VM_NetworkPort $slist]

if {[info exists myServerInfo($ovmServerMasterIP,name)]} {
    set masterName $myServerInfo($ovmServerMasterIP,name)
} else {
    send_user "The server $ovmServerMasterIP cannot be found!\n"
    send_user "Exiting...\n"
    finishRun 4
}
if {[info exists myServerInfo($ovmServerSlaveIP,name)]} {
    set secondaryName $myServerInfo($ovmServerSlaveIP,name)
} else {
    send_user "The server $ovmServerSlaveIP cannot be found!\n"
    send_user "Exiting...\n"
    finishRun 4
}

set masterID $myServerInfo($ovmServerMasterIP,id)
set secondaryID $myServerInfo($ovmServerSlaveIP,id)

## verify initial config -- no LUNs mapped to the servers
array set myNewServerInfo [verifyInitialConfig $ovmServerMasterIP $ovmServerSlaveIP [array get myServerInfo]]
set masterlocals $myNewServerInfo($ovmServerMasterIP,haslocal)
set secondlocals $myNewServerInfo($ovmServerSlaveIP,haslocal)

send_user "\n---------------------------------------------------------------------------------\n"
send_user "GSIS-303: Unmap LUNs from each server"
send_user "\n---------------------------------------------------------------------------------\n"
set testStep 0

send_user "Waiting 60 seconds for the servers to catch up...\n"
sleep 60

send_user "\n## [incr testStep 1]. Rescanning the Physical Disks on $masterName...\n"
send "refreshStorageLayer Server id=$masterID\r"
validateCommandOutput "refreshStorageLayer command for $masterName"

send_user "Waiting 60 seconds for changes to be propagated to the manager...\n"
sleep 60

send_user "\n## [incr testStep 1]. Rescanning the Physical Disks on $secondaryName...\n"
send "refreshStorageLayer Server id=$secondaryID\r"
validateCommandOutput "refreshStorageLayer command for $secondaryName"

send_user "Waiting 60 seconds for changes to be propagated to the manager...\n"
sleep 60

array set diskList1 [getDisks $ovmServerMasterIP [array get myServerInfo]]
array set diskList2 [getDisks $ovmServerSlaveIP [array get myServerInfo]]

set expectedUnmap1 1
set expectedUnmap2 1

## Now iterate through the disklists, and check the status
## of each disk.  In 3.2.1, this is the only way to know 
## whether or not a disk is mapped via the CLI.  Unmapped
## LUNs can be deleted, because they're only being deleted
## from the manager's inventory list, not actually being
## deleted from the server.....

send_user "\nBeginning the process to verify whether each disk is in a mapped\n"
send_user "or unmapped state.  LUNs which are marked as unmapped or offline\n"
send_user "will be deleted from the disklist so that they are not accidentally\n"
send_user "used in future tests.\n"

send_user "\nVerifying LUNs on $masterName.  "
set dcount [deleteLUNs [array get diskList1]]
if {$dcount == $expectedUnmap1} {
    send_user "Successfully deleted $expectedUnmap1 LUNs from $masterName\n"
    send_user "Waiting 120 seconds for everything to settle....\n\n"
    sleep 120
} else {
    send_user "Expected $expectedUnmap1 unmapped LUNs on $masterName,"
    send_user "but found $dcount unmapped LUNs; all of which were deleted!\n"
    finishRun 4
}


send_user "\nVerifying LUNs on $secondaryName.  "
set dcount [deleteLUNs [array get diskList2]]
if {$dcount == $expectedUnmap2} {
    send_user "Successfully deleted $expectedUnmap2 LUNs from $secondaryName\n"
    send_user "Waiting 120 seconds for everything to settle....\n\n"
    sleep 120
} else {
    send_user "Expected $expectedUnmap2 unmapped LUNs on $secondaryName,"
    send_user "but found $dcount unmapped LUNs; all of which were deleted!\n"
    finishRun 4
}

finishRun 0
